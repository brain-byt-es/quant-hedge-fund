"""
Unified Sync Coordinator

The "Desired State" engine for institutional-grade data integrity.
Manages idempotent, prioritized, and incremental data synchronization.
"""

import asyncio
import time
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Set

import polars as pl
from loguru import logger

from qsconnect.client import Client


class UnifiedSyncCoordinator:
    """
    Centralized coordinator for all data ingestion tasks.
    Uses 'State-of-the-World' analysis to determine missing data gaps.
    """

    def __init__(self, client: Optional[Client] = None):
        self.client = client or Client()
        self.writer = self.client._db_proxy
        self._stats = {
            "new_prices": 0,
            "new_financials": 0,
            "new_assets": 0,
            "skipped_dead_symbols": 0
        }
        self.active_universe: List[str] = []
        self.negative_cache: Set[str] = set()

    async def sync_all(self):
        """Main entry point for one-click ingestion."""
        logger.info("ğŸš€ Initializing Unified Sync Coordinator...")
        
        # 1. State-of-the-World (SotW) Analysis
        await self._analyze_state_of_the_world()
        
        # 2. Level 1: Morning Sync (Critical Gaps)
        await self._sync_level_1_critical()
        
        # 3. Level 2: Universe Enrichment (Fundamentals)
        await self._sync_level_2_enrichment()
        
        # 4. Level 3: Historical Backfill
        await self._sync_level_3_backfill()
        
        logger.success("âœ… Unified Sync Sequence Complete.")
        return self._stats

    async def _analyze_state_of_the_world(self):
        """Performs a comprehensive scan to define the gap matrix."""
        logger.info("ğŸ” Analyzing State-of-the-World...")
        
        # Define Universe
        self.active_universe = self.client.get_active_universe()
        
        # Load Negative Cache (Failed Scans in last 30 days)
        failed_res = self.client.query("""
            SELECT symbol FROM failed_scans 
            WHERE timestamp > (CURRENT_TIMESTAMP - INTERVAL 30 DAY)
        """)
        if not failed_res.is_empty():
            self.negative_cache = set(failed_res["symbol"].to_list())
            logger.info(f"Negative Cache: {len(self.negative_cache)} symbols throttled.")

        # Identify Price Gaps
        price_health = self.client.query("""
            SELECT symbol, MAX(date) as last_date, COUNT(*) as count
            FROM historical_prices_fmp 
            GROUP BY symbol
        """)
        
        # TODO: More complex gap analysis per symbol
        logger.info(f"SotW Analysis complete for {len(self.active_universe)} symbols.")

    async def _sync_level_1_critical(self):
        """Level 1: Daily OHLCV and new filings for active strategies."""
        logger.info("ğŸ“¡ Level 1: Syncing Critical Market Data...")
        
        # Filter universe by negative cache
        targets = [s for s in self.active_universe if s not in self.negative_cache]
        
        # Incremental Price Download
        # The Client.bulk_historical_prices already has some deduplication logic
        self.client.bulk_historical_prices(symbols=targets)
        
        # Sync Master Assets (FinanceDatabase)
        from qsconnect.ingestion.finance_db_sync import FinanceDatabaseSync
        sync = FinanceDatabaseSync(self.writer)
        sync.sync_all()

    async def _sync_level_2_enrichment(self):
        """Level 2: Quarterly fundamentals for the SimFin 5000+ universe."""
        logger.info("ğŸ“Š Level 2: Enriching Universe Fundamentals...")
        
        # Fetch symbols missing fundamentals in the last 90 days
        missing_fund_sql = """
            SELECT symbol FROM stock_list_fmp s
            WHERE symbol NOT IN (
                SELECT DISTINCT symbol FROM bulk_income_quarter_fmp 
                WHERE updated_at > (CURRENT_TIMESTAMP - INTERVAL 90 DAY)
            )
            LIMIT 500
        """
        targets = self.client.query(missing_fund_sql)
        if not targets.is_empty():
            target_list = targets["symbol"].to_list()
            # Use FMP Client to fetch in batches
            for i in range(0, len(target_list), 50):
                batch = target_list[i:i+50]
                logger.info(f"Syncing fundamentals batch for {len(batch)} symbols...")
                # Mock call: client handles the low-level FMP request
                # self.client.fetch_bulk_financial_statements(symbols=batch)
                await asyncio.sleep(1) # Rate limit protection

    async def _sync_level_3_backfill(self):
        """Level 3: Long-term historical backfill."""
        logger.info("â³ Level 3: Processing Historical Backfill...")
        # Placeholder for 10-year deep dives
        pass

    def get_progress(self) -> Dict[str, Any]:
        return self._stats
